    Num examples = 2,762
    Num Epochs = 8
    Instantaneous batch size per device = 16
    Total train batch size (w. parallel, distributed & accumulation) = 192
    Gradient Accumulation steps = 12
    Total optimization steps = 100
    Number of trainable parameters = 26,214,400
    %of parameters trained using LORA 26214400/2780000000 = 0.9%
    